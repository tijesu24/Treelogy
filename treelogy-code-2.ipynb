{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14643002,"sourceType":"datasetVersion","datasetId":9354058}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Cell 1: Imports & Configuration","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport joblib\nfrom glob import glob\nfrom tqdm.auto import tqdm\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom torch.utils.data import Dataset, DataLoader\nfrom IPython.display import FileLink\n\n# --- CONFIGURATION ---\nDATA_DIR = \"/kaggle/input/treelogy/train_images\"    # Make sure your folders are here: dataset/Species_Name/image.jpg\ntest_folder_dir = \"/kaggle/input/treelogy/test_images\"\nIMG_SIZE = 256          # The paper scales images to 256px [cite: 87]\nBATCH_SIZE = 32         # Batch size for CNN extraction\nNUM_WORKERS = 2         # CPU cores for parallel preprocessing (Kaggle usually allows 2-4)\n\n# Setup Device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Running on: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:14:45.608092Z","iopub.execute_input":"2026-01-28T09:14:45.608438Z","iopub.status.idle":"2026-01-28T09:14:45.616857Z","shell.execute_reply.started":"2026-01-28T09:14:45.608397Z","shell.execute_reply":"2026-01-28T09:14:45.616089Z"}},"outputs":[{"name":"stdout","text":"Running on: cuda\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Cell 2: Preprocessing Logic (The Core Algorithm)","metadata":{}},{"cell_type":"code","source":"def preprocess_leaf(img_path):\n    \"\"\"\n    Reads an image, removes background using K-Means in LAB space, \n    and removes the stem using morphological opening.\n    \"\"\"\n    # 1. Load Image\n    img = cv2.imread(img_path)\n    if img is None: return None\n    \n    # Resize (Longer edge -> 256px) [cite: 87]\n    h, w = img.shape[:2]\n    scale = IMG_SIZE / max(h, w)\n    img = cv2.resize(img, (int(w * scale), int(h * scale)))\n    \n    # 2. Background Elimination (K-Means in LAB color space) [cite: 81-86]\n    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)\n    pixels = img_lab.reshape((-1, 3))\n    \n    # k=2 clusters (leaf vs background)\n    kmeans = KMeans(n_clusters=2, n_init=3, random_state=42)\n    labels = kmeans.fit_predict(pixels)\n    \n    # Assume the leaf is the center pixel's cluster\n    center_idx = len(labels) // 2\n    leaf_label = labels[center_idx]\n    \n    # Create Binary Mask\n    mask = (labels == leaf_label).reshape(img.shape[:2]).astype(np.uint8) * 255\n    \n    # 3. Refine Mask (Gaussian Blur + Otsu) [cite: 91-93]\n    mask = cv2.GaussianBlur(mask, (5, 5), 0)\n    _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # 4. Stem Removal (Morphological Opening with Ellipse Kernel) [cite: 106-116]\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))\n    mask_final = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n    \n    # Apply mask to original image\n    result = cv2.bitwise_and(img, img, mask=mask_final)\n    \n    # Convert BGR (OpenCV default) to RGB (PyTorch default)\n    return cv2.cvtColor(result, cv2.COLOR_BGR2RGB)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:39:53.368029Z","iopub.execute_input":"2026-01-28T08:39:53.368432Z","iopub.status.idle":"2026-01-28T08:39:53.375128Z","shell.execute_reply.started":"2026-01-28T08:39:53.368406Z","shell.execute_reply":"2026-01-28T08:39:53.374605Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Cell 3: CNN Model (AlexNet Feature Extractor)","metadata":{}},{"cell_type":"code","source":"# Load pre-trained AlexNet\nbase_model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\nbase_model.to(device)\nbase_model.eval() # Set to evaluation mode\n\nclass TreelogyExtractor(torch.nn.Module):\n    def __init__(self, original_model):\n        super().__init__()\n        self.features = original_model.features\n        self.avgpool = original_model.avgpool\n        # Isolate up to fc6 (Index 1 in PyTorch's classifier block)\n        # Paper found fc6 + SVM gave best results (90.5%) [cite: 253]\n        self.classifier = torch.nn.Sequential(*list(original_model.classifier.children())[:2])\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\nextractor = TreelogyExtractor(base_model).to(device)\n\n# Standard ImageNet normalization for CNN input\ncnn_preprocess = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T08:39:53.375835Z","iopub.execute_input":"2026-01-28T08:39:53.376067Z","iopub.status.idle":"2026-01-28T08:39:55.555146Z","shell.execute_reply.started":"2026-01-28T08:39:53.376047Z","shell.execute_reply":"2026-01-28T08:39:55.554571Z"}},"outputs":[{"name":"stdout","text":"Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 233M/233M [00:01<00:00, 215MB/s] \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Cell 4: Optimized Data Loader (For Speed)","metadata":{}},{"cell_type":"code","source":"# --- ENHANCED DATA LOADER WITH ROTATION  ---\nclass AugmentedLeafDataset(Dataset):\n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n        \n    def __len__(self):\n        # We pretend the dataset is 4x larger (0, 90, 180, 270 degrees)\n        return len(self.file_list) * 4\n    \n    def __getitem__(self, idx):\n        # 1. Determine which image and which rotation\n        real_idx = idx // 4           # The actual image index\n        rotation_type = idx % 4       # 0=0°, 1=90°, 2=180°, 3=270°\n        \n        path, label = self.file_list[real_idx]\n        \n        # 2. Preprocess (CPU)\n        clean_leaf_rgb = preprocess_leaf(path)\n        if clean_leaf_rgb is None: return None\n        \n        # 3. Apply Rotation\n        if rotation_type == 1:\n            clean_leaf_rgb = cv2.rotate(clean_leaf_rgb, cv2.ROTATE_90_CLOCKWISE)\n        elif rotation_type == 2:\n            clean_leaf_rgb = cv2.rotate(clean_leaf_rgb, cv2.ROTATE_180)\n        elif rotation_type == 3:\n            clean_leaf_rgb = cv2.rotate(clean_leaf_rgb, cv2.ROTATE_90_COUNTERCLOCKWISE)\n        # rotation_type 0 does nothing (original image)\n\n        # 4. Transform for CNN\n        if self.transform:\n            img_tensor = self.transform(clean_leaf_rgb)\n        else:\n            img_tensor = clean_leaf_rgb\n            \n        return img_tensor, label\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T09:37:50.764855Z","iopub.execute_input":"2026-01-28T09:37:50.765567Z","iopub.status.idle":"2026-01-28T09:37:50.771525Z","shell.execute_reply.started":"2026-01-28T09:37:50.765537Z","shell.execute_reply":"2026-01-28T09:37:50.770703Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Cell 5: Main Training Loop","metadata":{}},{"cell_type":"code","source":"# 1. Gather all file paths\nall_files = []\nclasses = sorted([d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))])\nprint(f\"Found {len(classes)} classes: {classes}\")\n\nfor species in classes:\n    species_path = os.path.join(DATA_DIR, species)\n    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n        for img_path in glob(os.path.join(species_path, ext)):\n            all_files.append((img_path, species))\n\nprint(f\"Total images to process: {len(all_files)}\")\n\n# 2. Setup Loader\ntrain_dataset = AugmentedLeafDataset(all_files, transform=cnn_preprocess)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                    num_workers=NUM_WORKERS, collate_fn=collate_fn)\n\nprint(f\"Dataset virtually expanded to {len(train_dataset)} images via rotation.\")\n\n# 3. Extract Features\nX = []\ny = []\n\nprint(\"Starting Feature Extraction...\")\nwith torch.no_grad():\n    # tqdm creates the progress bar\n    for images, labels in tqdm(train_loader, desc=\"Processing Batches\"):\n        if images.numel() == 0: continue\n        \n        # Move batch to GPU\n        images = images.to(device)\n        \n        # Forward pass (Get fc6 vectors)\n        features = extractor(images)\n        \n        # Store results\n        X.append(features.cpu().numpy())\n        y.extend(labels)\n\n# Concatenate all batches\nX = np.concatenate(X, axis=0)\ny = np.array(y)\nprint(f\"Extraction Complete. Feature Matrix: {X.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:23:05.196187Z","iopub.execute_input":"2026-01-28T11:23:05.196545Z"}},"outputs":[{"name":"stdout","text":"Found 57 classes: ['n01440000', 'n01448192', 'n01456384', 'n01464576', 'n01472768', 'n01480960', 'n01489152', 'n01497344', 'n01505536', 'n01513728', 'n01521920', 'n01530112', 'n01538304', 'n01546496', 'n01554688', 'n01562880', 'n01571072', 'n01579264', 'n01587456', 'n01595648', 'n01603840', 'n01612032', 'n01620224', 'n01628416', 'n01636608', 'n01644800', 'n01652992', 'n01661184', 'n01669376', 'n01677568', 'n01685760', 'n01693952', 'n01702144', 'n01710336', 'n01718528', 'n01726720', 'n01734912', 'n01743104', 'n01751296', 'n01759488', 'n01767680', 'n01775872', 'n01784064', 'n01792256', 'n01800448', 'n01808640', 'n01816832', 'n01825024', 'n01833216', 'n01841408', 'n01849600', 'n01857792', 'n01865984', 'n01874176', 'n01882368', 'n01890560', 'n01898752']\nTotal images to process: 18992\nDataset virtually expanded to 75968 images via rotation.\nStarting Feature Extraction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Batches:   0%|          | 0/2374 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"444d4b27c2b444cf83b99974080ad8d9"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Cell 6: Train SVM & Save Model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# 1. Use the current X and y (assuming they are still in memory)\n# If you lost them, you sadly have to re-run the extraction steps.\n\n# 2. Split Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 3. Use SGDClassifier (The Fast \"Approximate\" SVM)\n# loss='hinge' makes it behave exactly like a Linear SVM\n# alpha=0.0001 is the regularization (similar to C in SVM)\n# n_jobs=-1 uses all CPU cores\nfast_svm = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3, n_jobs=-1, random_state=42)\n\n# Optional: Wrap in CalibratedClassifierCV if you want probability scores later\nclf = make_pipeline(StandardScaler(), fast_svm)\n\nprint(\"Fitting Fast SVM Classifier...\")\nclf.fit(X_train, y_train)\n\n# 4. Evaluate\npreds = clf.predict(X_test)\nacc = accuracy_score(y_test, preds)\nprint(f\"\\n========================================\")\nprint(f\"Final Accuracy: {acc*100:.2f}%\")\nprint(f\"========================================\")\n\n# 5. Save Model & Labels (So you can sleep!)\njoblib.dump(clf, 'treelogy_svm_model.pkl')\njoblib.dump(classes, 'class_labels.pkl')\nprint(\"Model saved as 'treelogy_svm_model.pkl'\")\n\ndisplay(FileLink('treelogy_svm_model.pkl'))\ndisplay(FileLink('class_labels.pkl'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:05:38.458722Z","iopub.execute_input":"2026-01-28T11:05:38.459397Z","iopub.status.idle":"2026-01-28T11:09:10.734250Z","shell.execute_reply.started":"2026-01-28T11:05:38.459346Z","shell.execute_reply":"2026-01-28T11:09:10.733608Z"}},"outputs":[{"name":"stdout","text":"Fitting Fast SVM Classifier...\n\n========================================\nFinal Accuracy: 77.52%\n========================================\nModel saved as 'treelogy_svm_model.pkl'\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/treelogy_svm_model.pkl","text/html":"<a href='treelogy_svm_model.pkl' target='_blank'>treelogy_svm_model.pkl</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/class_labels.pkl","text/html":"<a href='class_labels.pkl' target='_blank'>class_labels.pkl</a><br>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# Cell 7: (Optional) Test/Inference Code","metadata":{}},{"cell_type":"markdown","source":"## Test loop","metadata":{}},{"cell_type":"code","source":"# 1. Gather all file paths\nall_files = []\nclasses = sorted([d for d in os.listdir(test_folder_dir) if os.path.isdir(os.path.join(test_folder_dir, d))])\nprint(f\"Found {len(classes)} classes: {classes}\")\n\nfor species in classes:\n    species_path = os.path.join(test_folder_dir, species)\n    for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n        for img_path in glob(os.path.join(species_path, ext)):\n            all_files.append((img_path, species))\n\nprint(f\"Total images to process: {len(all_files)}\")\n\n# Standard Dataset (No Rotation/Augmentation)\nclass LeafDataset(Dataset):\n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, idx):\n        path, label = self.file_list[idx]\n        \n        # Preprocess (CPU)\n        clean_leaf_rgb = preprocess_leaf(path)\n        \n        if clean_leaf_rgb is None:\n            return None # Handle bad images\n            \n        # Transform for CNN\n        if self.transform:\n            img_tensor = self.transform(clean_leaf_rgb)\n        else:\n            img_tensor = clean_leaf_rgb\n            \n        return img_tensor, label\n\n# Helper to skip failed images\ndef collate_fn(batch):\n    batch = [item for item in batch if item is not None]\n    if len(batch) == 0:\n        return torch.tensor([]), [] \n    return torch.utils.data.dataloader.default_collate(batch)\n    \n\n# 2. Setup Loader\ntest_dataset = LeafDataset(all_files, transform=cnn_preprocess)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                    num_workers=NUM_WORKERS, collate_fn=collate_fn)\n\n# 3. Extract Features\nX_test = []\ny_test = []\n\nprint(\"Starting Feature Extraction...\")\nwith torch.no_grad():\n    # tqdm creates the progress bar\n    for images, labels in tqdm(test_loader, desc=\"Processing Batches\"):\n        if images.numel() == 0: continue\n        \n        # Move batch to GPU\n        images = images.to(device)\n        \n        # Forward pass (Get fc6 vectors)\n        features = extractor(images)\n        \n        # Store results\n        X_test.append(features.cpu().numpy())\n        y_test.extend(labels)\n\n# Concatenate all batches\nX_test = np.concatenate(X_test, axis=0)\ny_test = np.array(y_test)\nprint(f\"Extraction Complete. Feature Matrix: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:13:34.183361Z","iopub.execute_input":"2026-01-28T11:13:34.183663Z","iopub.status.idle":"2026-01-28T11:16:29.907136Z","shell.execute_reply.started":"2026-01-28T11:13:34.183638Z","shell.execute_reply":"2026-01-28T11:16:29.906271Z"}},"outputs":[{"name":"stdout","text":"Found 57 classes: ['t01440000', 't01448192', 't01456384', 't01464576', 't01472768', 't01480960', 't01489152', 't01497344', 't01505536', 't01513728', 't01521920', 't01530112', 't01538304', 't01546496', 't01554688', 't01562880', 't01571072', 't01579264', 't01587456', 't01595648', 't01603840', 't01612032', 't01620224', 't01628416', 't01636608', 't01644800', 't01652992', 't01661184', 't01669376', 't01677568', 't01685760', 't01693952', 't01702144', 't01710336', 't01718528', 't01726720', 't01734912', 't01743104', 't01751296', 't01759488', 't01767680', 't01775872', 't01784064', 't01792256', 't01800448', 't01808640', 't01816832', 't01825024', 't01833216', 't01841408', 't01849600', 't01857792', 't01865984', 't01874176', 't01882368', 't01890560', 't01898752']\nTotal images to process: 2640\nStarting Feature Extraction...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Batches:   0%|          | 0/83 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32ab4d75ed0a456baa36c8096d6f9647"}},"metadata":{}},{"name":"stdout","text":"Extraction Complete. Feature Matrix: (2640, 4096)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"\n\npreds = clf.predict(X_test)\n# Create a new list where we replace the starting 't' with 'n'\n# This converts 't01808640' -> 'n01808640' so it matches the model's output\ny_test_fixed = [label.replace('t', 'n', 1) if label.startswith('t') else label for label in y_test]\nacc = accuracy_score(y_test_fixed, preds)\n\nprint(f\"\\n========================================\")\nprint(f\"Testing Accuracy: {acc*100:.2f}%\")\nprint(f\"========================================\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:17:50.220838Z","iopub.execute_input":"2026-01-28T11:17:50.221234Z","iopub.status.idle":"2026-01-28T11:17:50.357350Z","shell.execute_reply.started":"2026-01-28T11:17:50.221182Z","shell.execute_reply":"2026-01-28T11:17:50.356113Z"}},"outputs":[{"name":"stdout","text":"\n========================================\nTesting Accuracy: 67.23%\n========================================\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import models\nfrom tqdm.auto import tqdm\n\n# --- CONFIGURATION ---\nNUM_EPOCHS = 10     \nLEARNING_RATE = 0.001\nBATCH_SIZE = 32\n\n# 1. Setup Data Loaders (Use your Augmented Dataset!)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n                          num_workers=2, collate_fn=collate_fn)\n\n# 2. Prepare the Model\nprint(\"Building Fine-Tuning Model...\")\nmodel = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n\n# Freeze features\nfor param in model.features.parameters():\n    param.requires_grad = False\n\n# Replace Classifier\nnum_features = model.classifier[6].in_features\nnum_classes = len(classes)\nmodel.classifier[6] = nn.Linear(num_features, num_classes)\n\nmodel = model.to(device)\n\n# 3. Define Training Tools\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n\n# 4. The Fine-Tuning Loop\nprint(f\"Starting Fine-Tuning for {NUM_EPOCHS} epochs...\")\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\"):\n        if images.numel() == 0: continue\n        \n        images = images.to(device)\n        \n        # Convert string labels to indices\n        label_indices = torch.tensor([classes.index(l) for l in labels]).to(device)\n        \n        # Zero gradients\n        optimizer.zero_grad()\n        \n        # Forward + Backward\n        outputs = model(images)\n        loss = criterion(outputs, label_indices)\n        loss.backward()\n        optimizer.step()\n        \n        # Stats\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        \n        # --- FIX IS HERE ---\n        # We calculate size first, then add to total\n        batch_len = label_indices.size(0)\n        total += batch_len\n        # -------------------\n        \n        correct += (predicted == label_indices).sum().item()\n        \n    epoch_acc = 100 * correct / total\n    print(f\"Epoch {epoch+1} Result: Loss={running_loss/len(train_loader):.4f}, Acc={epoch_acc:.2f}%\")\n\nprint(\"Fine-Tuning Complete! The model is now a 'Leaf Specialist'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T11:19:27.766146Z","iopub.execute_input":"2026-01-28T11:19:27.766678Z","iopub.status.idle":"2026-01-28T11:19:27.778278Z","shell.execute_reply.started":"2026-01-28T11:19:27.766650Z","shell.execute_reply":"2026-01-28T11:19:27.777387Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/785829799.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 1. Setup Data Loaders (Use your Augmented Dataset!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n\u001b[0m\u001b[1;32m     15\u001b[0m                           num_workers=2, collate_fn=collate_fn)\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"],"ename":"NameError","evalue":"name 'train_dataset' is not defined","output_type":"error"}],"execution_count":24}]}